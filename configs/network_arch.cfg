# ---------- pre ----------
[input]
height=448
width=336
filters=3

[padding]
pad_type=zero
pad=3

# upsample: 3 -> 64
[convolutional]
name=conv1
filters=64
size=7
stride=2
pad=0
batch_normalize=1
activation=relu

[pooling]
pool_type=maxpool2d
size=3
stride=2
pad=0
padding=same

# resnet
# ---------- stage1 -----------
# res1_a
[convolutional]
filters=64
size=1  
stride=1
pad=0
batch_normalize=1
activation=relu

[convolutional]
filters=64
size=3  
stride=1
pad=0
padding=same
dilation_rate=1
batch_normalize=1
activation=relu

# upsample: 64 -> 256
[convolutional]
filters=256
size=1  
stride=1
pad=0
batch_normalize=1
activation=None

[shortcut]
from=-4
convolutional=1
batch_normalize=1
filters=256
size=1
stride=1
pad=0
activation=relu

# res1_b
# downsample: 256 -> 64
[convolutional]
filters=64
size=1  
stride=1
pad=0
batch_normalize=1
activation=relu


[convolutional]
filters=64
size=3  
stride=1
pad=0
padding=same
dilation_rate=1
batch_normalize=1
activation=relu


# upsample: 64 -> 256
[convolutional]
filters=256
size=1  
stride=1
pad=0
batch_normalize=1
activation=None

[shortcut]
from=-4
convolutional=0
batch_normalize=0
activation=relu


# res_1c
# downsample: 256 -> 64
[convolutional]
filters=64
size=1  
stride=1
pad=0
batch_normalize=1
activation=relu


[convolutional]
filters=64
size=3  
stride=1
pad=0
padding=same
dilation_rate=1
batch_normalize=1
activation=relu


# upsample: 64 -> 256
[convolutional]
filters=256
size=1  
stride=1
pad=0
batch_normalize=1
activation=None

[shortcut]
from=-4
convolutional=0
batch_normalize=0
activation=relu


# --------------- stage2 -----------------
# res2_a
# downsample: 256 -> 128
[convolutional]
filters=128
size=1  
stride=2
pad=0
batch_normalize=1
activation=relu

[convolutional]
filters=128
size=3  
stride=1
pad=0
padding=same
dilation_rate=1
batch_normalize=1
activation=relu

# upsample: 128 -> 512
[convolutional]
filters=512
size=1  
stride=1
pad=0
batch_normalize=1
activation=None

[shortcut]
from=-4
convolutional=1
batch_normalize=1
filters=512
size=1
stride=2
pad=0
activation=relu

# res2_b
# downsample: 512 -> 128
[convolutional]
filters=128
size=1  
stride=1
pad=0
batch_normalize=1
activation=relu


[convolutional]
filters=128
size=3  
stride=1
pad=0
padding=same
dilation_rate=1
batch_normalize=1
activation=relu


# upsample: 128 -> 512
[convolutional]
filters=512
size=1  
stride=1
pad=0
batch_normalize=1
activation=None

[shortcut]
from=-4
convolutional=0
batch_normalize=0
activation=relu

# res2_c
# downsample: 512 -> 128
[convolutional]
filters=128
size=1  
stride=1
pad=0
batch_normalize=1
activation=relu


[convolutional]
filters=128
size=3  
stride=1
pad=0
padding=same
dilation_rate=1
batch_normalize=1
activation=relu


# upsample: 128 -> 512
[convolutional]
filters=512
size=1  
stride=1
pad=0
batch_normalize=1
activation=None

[shortcut]
from=-4
convolutional=0
batch_normalize=0
activation=relu


# res2_d
# downsample: 512 -> 128
[convolutional]
filters=128
size=1  
stride=1
pad=0
batch_normalize=1
activation=relu


[convolutional]
filters=128
size=3  
stride=1
pad=0
padding=same
dilation_rate=1
batch_normalize=1
activation=relu


# upsample: 128 -> 512
[convolutional]
filters=512
size=1  
stride=1
pad=0
batch_normalize=1
activation=None

[shortcut]
from=-4
convolutional=0
batch_normalize=0
activation=relu

# --------------- stage3 -----------------
# res3_a
# downsample: 512 -> 256
[convolutional]
filters=256
size=1  
stride=2
pad=0
batch_normalize=1
activation=relu

[convolutional]
filters=256
size=3  
stride=1
pad=0
padding=same
dilation_rate=1
batch_normalize=1
activation=relu

# upsample: 256 -> 1024
[convolutional]
filters=1024
size=1  
stride=1
pad=0
batch_normalize=1
activation=None

[shortcut]
from=-4
convolutional=1
batch_normalize=1
filters=1024
size=1
stride=2
pad=0
activation=relu

# res3_b
# downsample: 1024 -> 256
[convolutional]
filters=256
size=1  
stride=1
pad=0
batch_normalize=1
activation=relu


[convolutional]
filters=256
size=3  
stride=1
pad=0
padding=same
dilation_rate=1
batch_normalize=1
activation=relu


# upsample: 256 -> 1024
[convolutional]
filters=1024
size=1  
stride=1
pad=0
batch_normalize=1
activation=None

[shortcut]
from=-4
convolutional=0
batch_normalize=0
activation=relu

# res3_c
# downsample: 1024 -> 256
[convolutional]
filters=256
size=1  
stride=1
pad=0
batch_normalize=1
activation=relu


[convolutional]
filters=256
size=3  
stride=1
pad=0
padding=same
dilation_rate=1
batch_normalize=1
activation=relu


# upsample: 256 -> 1024
[convolutional]
filters=1024
size=1  
stride=1
pad=0
batch_normalize=1
activation=None

[shortcut]
from=-4
convolutional=0
batch_normalize=0
activation=relu


# res3_d
# downsample: 1024 -> 256
[convolutional]
filters=256
size=1  
stride=1
pad=0
batch_normalize=1
activation=relu


[convolutional]
filters=256
size=3  
stride=1
pad=0
padding=same
dilation_rate=1
batch_normalize=1
activation=relu


# upsample: 256 -> 1024
[convolutional]
filters=1024
size=1  
stride=1
pad=0
batch_normalize=1
activation=None

[shortcut]
from=-4
convolutional=0
batch_normalize=0
activation=relu

# res3_e
# downsample: 1024 -> 256
[convolutional]
filters=256
size=1  
stride=1
pad=0
batch_normalize=1
activation=relu


[convolutional]
filters=256
size=3  
stride=1
pad=0
padding=same
dilation_rate=1
batch_normalize=1
activation=relu


# upsample: 256 -> 1024
[convolutional]
filters=1024
size=1  
stride=1
pad=0
batch_normalize=1
activation=None

[shortcut]
from=-4
convolutional=0
batch_normalize=0
activation=relu

# res3_f
# downsample: 1024 -> 256
[convolutional]
filters=256
size=1  
stride=1
pad=0
batch_normalize=1
activation=relu


[convolutional]
filters=256
size=3  
stride=1
pad=0
padding=same
dilation_rate=1
batch_normalize=1
activation=relu


# upsample: 256 -> 1024
[convolutional]
filters=1024
size=1  
stride=1
pad=0
batch_normalize=1
activation=None

[shortcut]
from=-4
convolutional=0
batch_normalize=0
activation=relu


# --------------- stage4 -----------------
# res4_a
# downsample: 1024 -> 512
[convolutional]
filters=512
size=1  
stride=1
pad=0
batch_normalize=1
activation=relu

[convolutional]
filters=512
size=3  
stride=1
pad=0
padding=same
dilation_rate=2
batch_normalize=1
activation=relu

# upsample: 512 -> 2048
[convolutional]
filters=2048
size=1  
stride=1
pad=0
batch_normalize=1
activation=relu

[shortcut]
from=-4
convolutional=1
batch_normalize=1
filters=2048
size=1
stride=1
pad=0
activation=relu

# res4_b
# downsample: 2048 -> 512
[convolutional]
filters=512
size=1  
stride=1
pad=0
batch_normalize=1
activation=relu


[convolutional]
filters=512
size=3  
stride=1
pad=0
padding=same
dilation_rate=2
batch_normalize=1
activation=relu

# upsample: 512 -> 2048
[convolutional]
filters=2048
size=1  
stride=1
pad=0
batch_normalize=1
activation=relu

[shortcut]
from=-4
convolutional=0
batch_normalize=0
activation=relu

# res4_c
# downsample: 2048 -> 512
[convolutional]
filters=512
size=1  
stride=1
pad=0
batch_normalize=1
activation=relu


[convolutional]
filters=512
size=3  
stride=1
pad=0
padding=same
dilation_rate=2
batch_normalize=1
activation=relu

# upsample: 512 -> 2048
[convolutional]
filters=2048
size=1  
stride=1
pad=0
batch_normalize=1
activation=relu

[shortcut]
from=-4
convolutional=0
batch_normalize=0
activation=relu


# downsample: 2048 -> 256
# upsample: 256 -> 768
[route]
layers=-1, -13, -37
convolutional2d_transpose=1
l2_normalization=1
filters=256, 256, 256
size=4, 4, 4
stride=4, 4, 2
pad=0, 0, 1
kernel_initializer=xavier
gamma_init=10


# ----------------- post ---------------
# downsample: 768 -> 256
[convolutional]
filters=256
size=3  
stride=1
pad=0
padding=same
kernel_initializer=xavier
batch_normalize=1
activation=relu

# class
[output]
from=-1
filters=1
size=1
stride=1
pad=0
kernel_initializer=xavier
bias_initializer=
activation=sigmoid


# center
[output]
from=-2
filters=1
size=1
stride=1
pad=0
kernel_initializer=xavier
activation=linear
# 84 * 112 
input_size=9408
# 84 * 112 
output_size=9408

# offset
[output]
from=-3
filters=2
size=1
stride=1
pad=0
kernel_initializer=xavier
activation=linear
# 84 * 112 
input_size=9408
# 84 * 112 
output_size=9408